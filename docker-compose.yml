version: '3.8'

services:
  # GPU-enabled service with official backend (default)
  qwen3-tts-gpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    container_name: qwen3-tts-api
    ports:
      - "7654:7654"
    environment:
      - HOST=0.0.0.0
      - PORT=7654
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
      - TTS_WARMUP_ON_START=false
      - CUDA_VISIBLE_DEVICES=7
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    volumes:
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7654/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s

  # GPU-enabled service with vLLM-Omni backend
  qwen3-tts-vllm:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: qwen3-tts-api-vllm
    ports:
      - "7654:7654"
    environment:
      - HOST=0.0.0.0
      - PORT=7654
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=vllm_omni
      - TTS_WARMUP_ON_START=true
      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-0.6B-CustomVoice
#      - TTS_MODEL_NAME=Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice
      - VLLM_WORKER_MULTIPROC_METHOD=spawn
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - HF_HOME=/root/.cache/huggingface
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      # Mount model cache for persistence
      - ~/.cache/huggingface:/root/.cache/huggingface
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["7"]
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7654/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - vllm

  # CPU-only service
  qwen3-tts-cpu:
    build:
      context: .
      dockerfile: Dockerfile
      target: cpu-base
    container_name: qwen3-tts-api-cpu
    network_mode: host
    ports:
      - "7654:7654"
    environment:
      - HOST=0.0.0.0
      - PORT=7654
      - WORKERS=1
      - CORS_ORIGINS=*
      - TTS_BACKEND=official
    volumes:
      - ~/.cache/huggingface:/home/appuser/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7654/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 180s
    profiles:
      - cpu

# To run GPU version with official backend: docker-compose up qwen3-tts-gpu
# To run GPU version with vLLM backend: docker-compose --profile vllm up qwen3-tts-vllm
# To run CPU version: docker-compose --profile cpu up qwen3-tts-cpu
